{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen de las sentencias empleadas en el curso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar librerías recomendadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai # Instala la librería de OpenAI\n",
    "!pip install langchain==0.1.20 -Uq # Instala la librería de LangChain\n",
    "!pip install langchain-openai==0.1.3 -Uq # Instala la librería de LangChain para OpenAI\n",
    "!pip install langchain-groq==0.1.2 -Uq # Instala la librería de LangChain para Groq\n",
    "!pip install langchain-anthropic==0.1.12 -Uq # Instala la librería de LangChain para Anthropic\n",
    "\n",
    "!pip install pypdf -Uq # Instala la librería de PyPDF\n",
    "!pip install faiss-cpu -Uq # Instala la librería de Faiss como BD vectorial, se puede cambiar por faiss-gpu\n",
    "!pip install beautifulsoup4 # Instala la librería para hacer scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeras pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo probando directamente la API de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY) # Importar la API Key de OpenAI\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Eres un cocinero experto en recetas orientales\"},\n",
    "    {\"role\": \"user\", \"content\": \"Por favor explícame sobre la importancia de la disciplina a la hora de cocinar\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizando una prompt simple con LangChain a OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", api_key=openai_api_key) # Importar la API Key de OpenAI\n",
    "prompt = \"Hola, ¿cómo estás? ¿Con qué modelo de lenguaje tengo el gusto de estar hablando? ¿Exactamente qué modelo sos?\"\n",
    "respuesta = llm.invoke(prompt)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizando una prompt simple con LangChain a Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0, api_key=groq_api_key) # Importar la API Key de Groq\n",
    "prompt = \"Hola, ¿cómo estás? ¿Con qué modelo de lenguaje tengo el gusto de estar hablando? ¿Exactamente qué modelo sos?\"\n",
    "respuesta = llm.invoke(prompt)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizando una prompt simple con LangChain a Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model='claude-3-opus-20240229', temperature=0, api_key=anthropic_api_key) # Importar la API Key de Anthropic\n",
    "prompt = \"Hola, ¿cómo estás? ¿Con qué modelo de lenguaje tengo el gusto de estar hablando? ¿Exactamente qué modelo sos?\"\n",
    "respuesta = llm.invoke(prompt)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manejo de cadenas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de cadena simple con LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "texto=\"¡Hola mundo!\"\n",
    "template = \"Traduce del español al {idioma} el siguiente texto: '{texto}'\"\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"idioma\",\"texto\"], template=template\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", api_key=openai_api_key) # Importar la API key de OpenAI\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "respuesta = chain.invoke(input={\"idioma\":\"italiano\",\"texto\":texto})\n",
    "print(respuesta[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de cadena secuencial con LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "\"\"\"\n",
    "En el ejemplo se utilizan dos modelos distintos para cada cadena, la salida de la primera es la entrada de\n",
    "de la segunda. Además de llamo de una forma mas sencilla a PromptTemplate con from_template\n",
    "\"\"\"\n",
    "\n",
    "template1 = \"Genérame el código de una función en Python que realice lo siguiente: '{accion}'\"\n",
    "prompt_template1 = PromptTemplate.from_template(template1)\n",
    "llm1 = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", api_key=openai_api_key) # Importar la API key de OpenAI\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "template2 = \"Dada una {función}, por verifica si se encuentra correctamente codificada, y explicame lo que hace\"\n",
    "prompt_template2 = PromptTemplate.from_template(template2)\n",
    "llm2 = ChatGroq(model=\"llama3-70b-8192\", temperature=0, api_key=groq_api_key) # Importar la API key de Groq\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "cadena_secuencial = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "salida = cadena_secuencial.invoke(\"Calcula todos los números primos existentes hasta 100\")\n",
    "print(salida['output'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manejo de memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Buffer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\"\"\"\n",
    "Es importante de tener en cuenta que para manejar memoria debemos invocar una cadena especial. Para el ejemplo\n",
    "es ConversationChain\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatAnthropic(model='claude-3-opus-20240229', temperature=0, api_key=anthropic_api_key) # Importar la API key de Anthropic\n",
    "\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = buffer_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.invoke(input=\"Hola, mi nombre es Juan y me gusta ver partidos de futbol. La semana pasada vi 6 partidos completos.\")\n",
    "conversation.invoke(input=\"En los dos primeros días de esta semana ya he visto 4 partidos. ¿Cómo Crees que voy respecto de la cantidad de partidos que vi la semana pasada?\")\n",
    "conversation.invoke(input=\"¿Recuerdas mi nombre?\")\n",
    "\n",
    "print(buffer_memory.buffer)\n",
    "\n",
    "summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation buffer window memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\"\"\"\n",
    "Es importante de tener en cuenta que para manejar memoria debemos invocar una cadena especial. Para el ejemplo\n",
    "es ConversationChain\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatAnthropic(model='claude-3-opus-20240229', temperature=0, api_key=anthropic_api_key) # Importar la API key de Anthropic\n",
    "\n",
    "window_memory = ConversationBufferWindowMemory(k=2) # K indica la cantida de mensajes que se van a recordar\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = window_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.invoke(input=\"Hola, mi nombre es Juan. ¿Te gusta el cine?\")\n",
    "conversation.invoke(input=\"¿Cuáles son tus películas favoritas?\")\n",
    "conversation.invoke(input=\"¿Te gusta el cine argentino? ¿Cuáles son tus películas favoritas?\")\n",
    "conversation.invoke(input=\"¿Recuerdas mi nombre?\")\n",
    "\n",
    "print(window_memory.buffer)\n",
    "\n",
    "summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation token buffer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\"\"\"\n",
    "Es importante de tener en cuenta que para manejar memoria debemos invocar una cadena especial. Para el ejemplo\n",
    "es ConversationChain\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatAnthropic(model='claude-3-opus-20240229', temperature=0, api_key=anthropic_api_key) # Importar la API key de Anthropic\n",
    "\n",
    "token_memory = ConversationTokenBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=500\n",
    ") # max_token_limit indica la cantidad de tokens que se van a recordar\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = token_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.invoke(input=\"Hola, mi nombre es Juan. ¿Te gusta el cine?\")\n",
    "conversation.invoke(input=\"¿Cuáles son tus películas favoritas?\")\n",
    "conversation.invoke(input=\"¿Te gusta el cine argentino? ¿Cuáles son tus películas favoritas?\")\n",
    "conversation.invoke(input=\"¿Recuerdas mi nombre?\")\n",
    "\n",
    "print(token_memory.buffer)\n",
    "\n",
    "summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation summary memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\"\"\"\n",
    "Es importante de tener en cuenta que para manejar memoria debemos invocar una cadena especial. Para el ejemplo\n",
    "es ConversationChain\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatAnthropic(model='claude-3-opus-20240229', temperature=0, api_key=anthropic_api_key) # Importar la API key de Anthropic\n",
    "\n",
    "summary_memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=100\n",
    ") # max_token_limit indica la cantidad de tokens que se van a resumir\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = summary_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.invoke(input=\"Hola, mi nombre es Juan. ¿Te gusta el cine?\")\n",
    "conversation.invoke(input=\"¿Cuáles son tus películas favoritas?\")\n",
    "conversation.invoke(input=\"¿Te gusta el cine argentino? ¿Cuáles son tus películas favoritas?\")\n",
    "conversation.invoke(input=\"¿Recuerdas mi nombre?\")\n",
    "\n",
    "print(summary_memory.buffer)\n",
    "\n",
    "summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo con lectura de PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\"\"\"\n",
    "Es importante de tener en cuenta que para manejar memoria debemos invocar una cadena especial. Para el ejemplo\n",
    "es RetrievalQA\n",
    "\"\"\"\n",
    "\n",
    "# Cargo el documento PDF\n",
    "loader = PyPDFLoader(\"/content/Anexo Curso_ Desarrollo de Aplicaciones LLM con LangChain, LlamaIndex y OpenAI.pdf\")\n",
    "documento = loader.load()\n",
    "# print(len(documento))\n",
    "\n",
    "# Divido el texto en fragmentos\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=400\n",
    ")\n",
    "document_chunks = text_splitter.split_documents(documento)\n",
    "#print(f\"Ahora tenemos {len(document_chunks)} partes.\")\n",
    "\n",
    "# Creo la BD de embeddings\n",
    "embeddings_model = OpenAIEmbeddings(api_key=openai_api_key) # Importar la API key de OpenAI\n",
    "stored_embeddings = FAISS.from_documents(document_chunks, embeddings_model)\n",
    "\n",
    "# Defino el modelo LLM\n",
    "llm = OpenAI(api_key=openai_api_key) # Importar la API key de OpenAI\n",
    "\n",
    "# Creo la cadena de consulta\n",
    "QA_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=stored_embeddings.as_retriever()\n",
    ")\n",
    "\n",
    "question = \"\"\"\n",
    "De qué trata el documento\n",
    "\"\"\"\n",
    "QA_chain.invoke(question)\n",
    "\n",
    "question2 = \"\"\"\n",
    "A quién va dirigido el curso?\n",
    "\"\"\"\n",
    "QA_chain.invoke(question2)\n",
    "\n",
    "question3 = \"\"\"\n",
    "Cuánto cuesta el curso?\n",
    "\"\"\"\n",
    "QA_chain.invoke(question3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo para hacer web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\"\"\"\n",
    "Es importante de tener en cuenta que para manejar memoria debemos invocar una cadena especial. Para el ejemplo\n",
    "es RetrievalQA\n",
    "\"\"\"\n",
    "\n",
    "# Carga el loader de scaneo de sitios webs\n",
    "loader = WebBaseLoader(\"https://www.infobae.com/\")\n",
    "datos_web = loader.load()\n",
    "#print(datos_web)\n",
    "\n",
    "# Divido el texto en fragmentos\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=400\n",
    ")\n",
    "datos_web_chunks = text_splitter.split_documents(datos_web)\n",
    "#print(f\"Ahora tenemos {len(datos_web_chunks)} partes.\")\n",
    "\n",
    "# Creo la BD de embeddings\n",
    "embeddings_model = OpenAIEmbeddings(api_key=openai_api_key) # Importar la API key de OpenAI\n",
    "stored_embeddings = FAISS.from_documents(datos_web_chunks, embeddings_model)\n",
    "\n",
    "# Defino el modelo LLM\n",
    "llm = OpenAI(api_key=openai_api_key) # Importar la API key de OpenAI\n",
    "\n",
    "# Creo la cadena de consulta\n",
    "QA_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=stored_embeddings.as_retriever()\n",
    ")\n",
    "\n",
    "pregunta1 = \"\"\"\n",
    "De qué trata la web?\n",
    "\"\"\"\n",
    "respuesta1 = QA_chain.invoke(pregunta1)\n",
    "print(respuesta1['result'])\n",
    "\n",
    "pregunta2 = \"\"\"\n",
    "Qué noticias de deportes hay??\n",
    "\"\"\"\n",
    "respuesta2 = QA_chain.invoke(pregunta2)\n",
    "print(respuesta2['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
