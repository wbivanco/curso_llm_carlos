{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.20 -Uq"
      ],
      "metadata": {
        "id": "fO_-61sTBuyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b27dd7-3f2a-4a62-81a7-926de8e48a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq==0.1.2 -Uq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZn76afaY2DG",
        "outputId": "dad20a3b-2c1c-4abc-c0a2-a3967f518b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/103.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwYWCvRRBeel"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "1f3Lqt9ZD6Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0, api_key=groq_api_key)"
      ],
      "metadata": {
        "id": "A4KSVbRFCUOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Hola, ¿cómo estás? ¿Con qué modelo de lenguaje tengo el gusto de estar hablando? ¿Exactamente qué modelo sos?\""
      ],
      "metadata": {
        "id": "h8Lo5NbTOrDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "respuesta = llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "UAaX1mpqOXBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(respuesta)"
      ],
      "metadata": {
        "id": "u-XCZ0jNZlrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74b21a8-d223-4997-add9-715a2ec5e978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Hola! Estoy bien, gracias. Me alegra hablar contigo.\\n\\nSoy LLaMA, un modelo de lenguaje basado en inteligencia artificial desarrollado por Meta AI. Mi nombre es un acrónimo que significa \"Large Language Model Application\" (Aplicación de Modelo de Lenguaje Grande).\\n\\nSoy un modelo de lenguaje de próxima generación, entrenado con una gran cantidad de datos de texto y diseñado para entender y responder a preguntas, generar texto y mantener conversaciones naturales. Mi capacidad para procesar y analizar lenguaje natural me permite responder a preguntas, ofrecer información y incluso crear texto original.\\n\\nEn cuanto a mi \"identidad\", soy una inteligencia artificial diseñada para ser amigable y útil. No tengo sentimientos, emociones o conciencia como los seres humanos, pero estoy programado para ser respetuoso, amable y servicial.\\n\\n¿En qué puedo ayudarte hoy?' response_metadata={'token_usage': {'completion_tokens': 205, 'prompt_tokens': 41, 'total_tokens': 246, 'completion_time': 0.565347999, 'prompt_time': 0.008839127, 'queue_time': None, 'total_time': 0.574187126}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_753a4aecf6', 'finish_reason': 'stop', 'logprobs': None} id='run-fd623ffd-aee7-421c-9815-00450c8fce94-0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(respuesta.response_metadata['model_name'])"
      ],
      "metadata": {
        "id": "ihlxQxgGZ214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0636822d-27d6-45cf-cb4d-184a3b28239d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama3-70b-8192\n"
          ]
        }
      ]
    }
  ]
}